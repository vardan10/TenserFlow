{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from yahoo_finance import Share                 # Library to gather stock prices\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "def get_prices(share_symbol, start_date, end_date, cache_filename='stock_prices.npy'):\n",
    "    try:\n",
    "        stock_prices = np.load(cache_filename)\n",
    "    except IOError:\n",
    "        share = Share(share_symbol)\n",
    "        stock_hist = share.get_historical(start_date, end_date)\n",
    "        stock_prices = [stock_price['Open'] for stock_price in stock_hist]\n",
    "        np.save(cache_filename, stock_prices)\n",
    "\n",
    "    return stock_prices.astype(float)\n",
    "\n",
    "\n",
    "def plot_prices(prices):\n",
    "    plt.title('Opening stock prices')\n",
    "    plt.xlabel('day')\n",
    "    plt.ylabel('price ($)')\n",
    "    plt.plot(prices)\n",
    "    plt.savefig('prices.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Most reinforcement learning algorithms follow similar implementation patterns.\n",
    "class DecisionPolicy:\n",
    "    def select_action(self, current_state, step):\n",
    "        pass\n",
    "\n",
    "    def update_q(self, state, action, reward, next_state):\n",
    "        pass\n",
    "\n",
    "# Next, let's inherit from this superclass to implement a random decision policy\n",
    "# which will randomly pick an action without even looking at the state.\n",
    "class RandomDecisionPolicy(DecisionPolicy):\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "\n",
    "    def select_action(self, current_state, step):\n",
    "        action = random.choice(self.actions)\n",
    "        return action\n",
    "\n",
    "\n",
    "class QLearningDecisionPolicy(DecisionPolicy):\n",
    "    def __init__(self, actions, input_dim):\n",
    "        \n",
    "        # To keep the solution from getting “stuck” when applying the same action over and over.\n",
    "        # The lesser its value, the more often it will randomly explore new actions.\n",
    "        self.epsilon = 0.95\n",
    "        \n",
    "        self.gamma = 0.3\n",
    "        self.actions = actions\n",
    "        output_dim = len(actions)      # output dimentions\n",
    "        h1_dim = 20                    # Set the number of hidden nodes in the neural networks\n",
    "\n",
    "        # Design of neural network\n",
    "        self.x = tf.placeholder(tf.float32, [None, input_dim])         # input vector\n",
    "        self.y = tf.placeholder(tf.float32, [output_dim])              # output vector\n",
    "        W1 = tf.Variable(tf.random_normal([input_dim, h1_dim]))        # weights from input to hidden layer\n",
    "        b1 = tf.Variable(tf.constant(0.1, shape=[h1_dim]))             # biases from input to hidden layer\n",
    "        h1 = tf.nn.relu(tf.matmul(self.x, W1) + b1)                    # hidden layer vector\n",
    "        W2 = tf.Variable(tf.random_normal([h1_dim, output_dim]))       # weights from hidden to output layer\n",
    "        b2 = tf.Variable(tf.constant(0.1, shape=[output_dim]))         # biases from hidden to output layer\n",
    "        self.q = tf.nn.relu(tf.matmul(h1, W2) + b2)                    # output of neural network\n",
    "\n",
    "        # Set loss as a squared error and use an optimizer\n",
    "        loss = tf.square(self.y - self.q)\n",
    "        self.train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def select_action(self, current_state, step):\n",
    "        threshold = min(self.epsilon, step / 1000.)\n",
    "        \n",
    "        if random.random() < threshold:\n",
    "            \n",
    "            # Exploit best option with probability epsilon\n",
    "            action_q_vals = self.sess.run(self.q, feed_dict={self.x: current_state})\n",
    "            action_idx = np.argmax(action_q_vals)\n",
    "            action = self.actions[action_idx]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Explore random option with probability 1 - epsilon\n",
    "            action = self.actions[random.randint(0, len(self.actions) - 1)]\n",
    "            \n",
    "        return action\n",
    "\n",
    "    \n",
    "    def update_q(self, state, action, reward, next_state):\n",
    "        action_q_vals = self.sess.run(self.q, feed_dict={self.x: state})\n",
    "        next_action_q_vals = self.sess.run(self.q, feed_dict={self.x: next_state})\n",
    "        next_action_idx = np.argmax(next_action_q_vals)\n",
    "        current_action_idx = self.actions.index(action)\n",
    "        action_q_vals[0, current_action_idx] = reward + self.gamma * next_action_q_vals[0, next_action_idx]\n",
    "        action_q_vals = np.squeeze(np.asarray(action_q_vals))\n",
    "        self.sess.run(self.train_op, feed_dict={self.x: state, self.y: action_q_vals})\n",
    "    \n",
    "    \n",
    "    \n",
    "def run_simulation(policy, initial_budget, initial_num_stocks, prices, hist, debug=False):\n",
    "    budget = initial_budget\n",
    "    num_stocks = initial_num_stocks\n",
    "    share_value = 0\n",
    "    transitions = list()\n",
    "    for i in range(len(prices) - hist - 1):\n",
    "        if i % 1000 == 0:\n",
    "            print('progress {:.2f}%'.format(float(100*i) / (len(prices) - hist - 1)))\n",
    "        current_state = np.asmatrix(np.hstack((prices[i:i+hist], budget, num_stocks)))\n",
    "        current_portfolio = budget + num_stocks * share_value\n",
    "        action = policy.select_action(current_state, i)\n",
    "        share_value = float(prices[i + hist])\n",
    "        if action == 'Buy' and budget >= share_value:\n",
    "            budget -= share_value\n",
    "            num_stocks += 1\n",
    "        elif action == 'Sell' and num_stocks > 0:\n",
    "            budget += share_value\n",
    "            num_stocks -= 1\n",
    "        else:\n",
    "            action = 'Hold'\n",
    "        new_portfolio = budget + num_stocks * share_value\n",
    "        reward = new_portfolio - current_portfolio\n",
    "        next_state = np.asmatrix(np.hstack((prices[i+1:i+hist+1], budget, num_stocks)))\n",
    "        transitions.append((current_state, action, reward, next_state))\n",
    "        policy.update_q(current_state, action, reward, next_state)\n",
    "\n",
    "    portfolio = budget + num_stocks * share_value\n",
    "    if debug:\n",
    "        print('${}\\t{} shares'.format(budget, num_stocks))\n",
    "    return portfolio\n",
    "\n",
    "\n",
    "# To obtain a more robust measurement of success, let's run the simulation a couple times and average the results.\n",
    "def run_simulations(policy, budget, num_stocks, prices, hist):\n",
    "    num_tries = 5\n",
    "    final_portfolios = list()\n",
    "    for i in range(num_tries):\n",
    "        print('Running simulation {}...'.format(i + 1))\n",
    "        final_portfolio = run_simulation(policy, budget, num_stocks, prices, hist)\n",
    "        final_portfolios.append(final_portfolio)\n",
    "        print('Final portfolio: ${}'.format(final_portfolio))\n",
    "    plt.title('Final Portfolio Value')\n",
    "    plt.xlabel('Simulation #')\n",
    "    plt.ylabel('Net worth')\n",
    "    plt.plot(final_portfolios)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    prices = get_prices('MSFT', '1992-07-22', '2016-07-22')\n",
    "    plot_prices(prices)\n",
    "    actions = ['Buy', 'Sell', 'Hold']\n",
    "    hist = 3\n",
    "    # policy = RandomDecisionPolicy(actions)\n",
    "    policy = QLearningDecisionPolicy(actions, hist + 2)\n",
    "    budget = 100000.0\n",
    "    num_stocks = 0\n",
    "    run_simulations(policy, budget, num_stocks, prices, hist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
